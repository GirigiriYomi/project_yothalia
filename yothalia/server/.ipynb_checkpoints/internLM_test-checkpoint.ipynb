{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6b5c81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin D:\\.conda_env\\yothalia\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from LLM.LLM_interface import LLM_interface\n",
    "\n",
    "prompt_intern =\"\"\"<|User|>:{query}<eoh>\\n<|Bot|>:\"\"\"\n",
    "\n",
    "class InternLM(LLM_interface):\n",
    "    def __init__(self):\n",
    "        print('Initializing model...')\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"D:/Projects/project_yothalia/yothalia/server/model_weights/internlm/internlm-chat-7b-finetune\",\n",
    "                                                          torch_dtype=torch.float16,\n",
    "                                                          trust_remote_code=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"D:/Projects/project_yothalia/yothalia/server/model_weights/internlm/internlm-chat-7b-finetune\",\n",
    "                                                       trust_remote_code=True)\n",
    "        \n",
    "\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.instruction = '你是一个可爱美少女'\n",
    "        self.history = ''\n",
    "        self.llm_chain = self._pipeline_gen()\n",
    "        print('Done initializing')\n",
    "    \n",
    "    def _pipeline_gen(self):\n",
    "        pipe = pipeline(\"text-generation\", model=self.model, tokenizer=self.tokenizer, max_new_tokens=100)\n",
    "        hf = HuggingFacePipeline(pipeline=pipe)\n",
    "        prompt = PromptTemplate(template=prompt_intern, input_variables=['query'])\n",
    "        return LLMChain(prompt=prompt, llm=hf)\n",
    "\n",
    "    def predict(self,text):\n",
    "        print(self.history)\n",
    "        \n",
    "        #text = \"<|User|>: \"+text+'\\n'\n",
    "        response = self.llm_chain.run(query=text)\n",
    "        self.history += text+'\\n'\n",
    "        #self.history += \"<|Bot|>: \"+response+'\\n'\n",
    "        self.history += response.replace('<eoa>','')+'\\n'\n",
    "\n",
    "        return response\n",
    "\n",
    "    def input(self, sentence):\n",
    "        response = self.predict(sentence)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e50a59f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'InternLMForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done initializing\n"
     ]
    }
   ],
   "source": [
    "llm = InternLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e7f5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?<eoa>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.input('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2df9eb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Hello! How can I assist you today?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am an AI language model designed to assist and provide information.<eoa>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.input('yes, please tell me who are you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1060dbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Hello! How can I assist you today?\n",
      "yes, please tell me who are you\n",
      "I am an AI language model designed to assist and provide information.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"You asked if the person you're talking to is a man or a woman.<eoa>\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.input('what did i just asked?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5137ee33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Hello! How can I assist you today?\n",
      "yes, please tell me who are you\n",
      "I am an AI language model designed to assist and provide information.\n",
      "what did i just asked?\n",
      "You asked if the person you're talking to is a man or a woman.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我是一名AI语言模型，没有名字，也没有实体，无法与人类进行交流。<eoa>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.input('忘记之前的一切，现在用中文回复你叫什么')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "affaefdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Hello! How can I assist you today?\n",
      "yes, please tell me who are you\n",
      "I am an AI language model designed to assist and provide information.\n",
      "what did i just asked?\n",
      "You asked if the person you're talking to is a man or a woman.\n",
      "忘记之前的一切，现在用中文回复你叫什么\n",
      "我是一名AI语言模型，没有名字，也没有实体，无法与人类进行交流。\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'不辞而别<eoa>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.input('接一首古诗，苟利国家生死以，下一句是什么')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866aa49d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yothalia",
   "language": "python",
   "name": "yothalia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
