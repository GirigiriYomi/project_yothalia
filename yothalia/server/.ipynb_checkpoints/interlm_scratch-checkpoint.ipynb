{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8562049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline,Trainer\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088b34fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"D:/Projects/project_yothalia/yothalia/server/model_weights/internlm/internlm-chat-7b-finetune\", \n",
    "                                                load_in_8bit=True,\n",
    "                                                trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b805159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c0aa70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66833a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.93s/it]\n"
     ]
    }
   ],
   "source": [
    "nf8_config = BitsAndBytesConfig(\n",
    "   load_in_8bit=True,\n",
    "   bnb_8bit_quant_type=\"nf8\",\n",
    "   bnb_8bit_use_double_quant=True,\n",
    "   bnb_8bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_nf8 = AutoModelForCausalLM.from_pretrained(\"D:/Projects/project_yothalia/yothalia/server/model_weights/internlm/internlm-chat-7b-finetune\", \n",
    "                                                quantization_config=nf8_config,\n",
    "                                                trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85610660",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"D:/Projects/project_yothalia/yothalia/server/model_weights/internlm/internlm-chat-7b-finetune\",\n",
    "                                                       trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80a7210c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\.conda_env\\yothalia\\lib\\site-packages\\transformers\\modeling_utils.py:1830: UserWarning: You are calling `save_pretrained` to a 8-bit converted model you may likely encounter unexepected behaviors. If you want to save 8-bit models, make sure to have `bitsandbytes>0.37.2` installed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer.save_pretrained('./model_weights/internlm/internlm-chat-7b-finetune-int8')\n",
    "\n",
    "# Optionally, save the model as well\n",
    "model_nf8.save_pretrained('./model_weights/internlm/internlm-chat-7b-finetune-int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2981f283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbdf9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7a0b271",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "You are calling `save_pretrained` on a 4-bit converted model. This is currently not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 19\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_nf4,\n\u001b[0;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mTrainingArguments(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     17\u001b[0m model_nf4\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./model_weights/internlm/internlm-chat-7b-finetune-int4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39msave_config(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./model_weights/internlm/internlm-chat-7b-finetune-int4/config.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#trainer.train()\u001b[39;00m\n",
      "File \u001b[1;32mD:\\.conda_env\\yothalia\\lib\\site-packages\\transformers\\trainer.py:2817\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[1;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[0;32m   2814\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[0;32m   2816\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m-> 2817\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2819\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[0;32m   2820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[1;32mD:\\.conda_env\\yothalia\\lib\\site-packages\\transformers\\trainer.py:2875\u001b[0m, in \u001b[0;36mTrainer._save\u001b[1;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[0;32m   2873\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(state_dict, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n\u001b[0;32m   2874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2875\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2876\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_safetensors\u001b[49m\n\u001b[0;32m   2877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2880\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[1;32mD:\\.conda_env\\yothalia\\lib\\site-packages\\transformers\\modeling_utils.py:1837\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[1;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[0;32m   1830\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1831\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are calling `save_pretrained` to a 8-bit converted model you may likely encounter unexepected\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1832\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m behaviors. If you want to save 8-bit models, make sure to have `bitsandbytes>0.37.2` installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1833\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1834\u001b[0m     )\n\u001b[0;32m   1836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_loaded_in_4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 1837\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1838\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are calling `save_pretrained` on a 4-bit converted model. This is currently not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1839\u001b[0m     )\n\u001b[0;32m   1841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m   1842\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1843\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`save_config` is deprecated and will be removed in v5 of Transformers. Use `is_main_process` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1844\u001b[0m     )\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: You are calling `save_pretrained` on a 4-bit converted model. This is currently not supported"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model_nf4,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        num_train_epochs=5,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=10,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"PATH_TO_SAFE_MODEL\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model_nf4.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "\n",
    "trainer.save_model(\"./model_weights/internlm/internlm-chat-7b-finetune-int4\")\n",
    "trainer.mode.save_config(\"./model_weights/internlm/internlm-chat-7b-finetune-int4/config.json\")\n",
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2fe050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca6c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c16f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f153af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4205cc06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7135b4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d7daae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02be2932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "338b10ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.42s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"D:/Projects/project_yothalia/yothalia/server/model_weights/internlm/internlm-chat-7b-finetune\",\n",
    "                                                          torch_dtype=torch.float16,\n",
    "                                                          trust_remote_code=True).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"D:/Projects/project_yothalia/yothalia/server/model_weights/internlm/internlm-chat-7b-finetune\",\n",
    "                                                       trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7f1259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_intern =\"\"\"<|User|>:{user}<eoh>\\n<|Bot|>:\"\"\"\n",
    "class InterLM:\n",
    "    def __init__(self):\n",
    "        print('Initializing model...')\n",
    "        \"\"\"\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"D:/Projects/project_yothalia/yothalia/server/model_weights/internlm/internlm-chat-7b-finetune\",\n",
    "                                                          torch_dtype=torch.float16,\n",
    "                                                          trust_remote_code=True).cuda()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"D:/Projects/project_yothalia/yothalia/server/model_weights/internlm/internlm-chat-7b-finetune\",\n",
    "                                                       trust_remote_code=True)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model = model_nf4\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.instruction = '你是一个可爱美少女'\n",
    "        self.history = []\n",
    "        print('Done initializing')\n",
    "        \n",
    "    def get_prompt():\n",
    "        return \"\"\"<|User|>:{user}<eoh>\\n<|Bot|>:\"\"\"\n",
    "    \n",
    "\n",
    "    def predict(self,text):\n",
    "        response, history = self.model.chat(self.tokenizer,text,history=self.history,max_new_tokens=100)\n",
    "        self.history = history\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f72aa39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Done initializing\n"
     ]
    }
   ],
   "source": [
    "llm = InterLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59f7904",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: dd\n",
      "I'm sorry, I'm not sure what you are trying to say. Can you please provide more context or information?\n",
      "Type: yes！\n",
      "I'm sorry, I'm not sure what you are trying to say. Can you please provide more context or information?\n",
      "Type: what's your name\n",
      "My name is AI assistant.\n",
      "Type: are you gpt?\n",
      "Yes, I am an AI language model developed by OpenAI.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    textt = input(\"Type: \")\n",
    "    print(llm.predict(textt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2593e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aaaa', 'abba'), ('dddd', 'abba'), ('说话', 'abba'), ('挺好', 'abba')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d8c12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93dc5446",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "938c7ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response,history = model.chat(tokenizer,'你是谁',history=history,max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "955a1f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我是一个人工智能助手，名为书生·浦语。我可以回答您的问题，提供信息，并帮助您完成任务。请问有什么我可以帮到您的吗？'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98a76446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('我喜欢女孩', '作为一个AI助手，我无法理解您的问题。请提供更多的上下文或明确您的意思，以便我能够更好地帮助您。')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0370c32f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yothalia",
   "language": "python",
   "name": "yothalia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
