{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b35a80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline,Trainer\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fca788aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nf8_config = BitsAndBytesConfig(\n",
    "   load_in_8bit=True,\n",
    "   bnb_8bit_quant_type=\"nf8\",\n",
    "   bnb_8bit_use_double_quant=True,\n",
    "   bnb_8bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.02,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\",\"lm_head\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f10813e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.89s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"D:/Projects/project_yothalia/yothalia/server/model_weights/internlm/internlm-chat-7b-finetune\", \n",
    "                                                quantization_config=nf8_config,\n",
    "                                                #peft_config=config,\n",
    "                                                trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48c4fdff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InternLMForCausalLM(\n",
       "  (model): InternLMModel(\n",
       "    (embed_tokens): Embedding(103172, 4096, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x InternLMDecoderLayer(\n",
       "        (self_attn): InternLMAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
       "          (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
       "          (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
       "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
       "          (rotary_emb): InternLMDynamicNTKScalingRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): InternLMMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): InternLMRMSNorm()\n",
       "        (post_attention_layernorm): InternLMRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): InternLMRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=103172, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2d510d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 41,693,248 || all params: 7,363,674,176 || trainable%: 0.5662016950164418\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfc24b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"D:/Projects/project_yothalia/yothalia/server/model_weights/internlm/internlm-chat-7b-finetune-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7b0e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c00c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>[INST]<<SYS>>\n",
    "{instruction}<</SYS>>\n",
    "{history}\n",
    "{user}[/INST]\n",
    "{assisstant}</s>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea76ce73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da87183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42add2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd0925c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c03f89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yothalia",
   "language": "python",
   "name": "yothalia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
